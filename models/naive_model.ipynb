{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10e9c1ad0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS device is available. Using MPS for acceleration.\n"
     ]
    }
   ],
   "source": [
    "# choose device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device is available. Using MPS for acceleration.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device is available. Using CUDA for acceleration.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU acceleration available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model A's forward function\n",
    "def model_A_forward(x, params):\n",
    "    # Convolutional layer\n",
    "    x = F.conv2d(x, params['conv1.weight'], params['conv1.bias'], stride=1, padding=0)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, 2)\n",
    "    # Flatten\n",
    "    x = x.view(x.size(0), -1)\n",
    "    # Fully connected layer\n",
    "    x = F.linear(x, params['fc.weight'], params['fc.bias'])\n",
    "    return x\n",
    "\n",
    "# Initialize Model A's parameters\n",
    "def initialize_model_A_params(device):\n",
    "    params = {\n",
    "        'conv1.weight': nn.Parameter(torch.randn(10, 1, 5, 5, device=device) * 0.1),\n",
    "        'conv1.bias': nn.Parameter(torch.zeros(10, device=device)),\n",
    "        'fc.weight': nn.Parameter(torch.randn(10, 1440, device=device) * 0.1),  # Adjusted size\n",
    "        'fc.bias': nn.Parameter(torch.zeros(10, device=device)),\n",
    "    }\n",
    "    return params\n",
    "\n",
    "# Flatten parameters with consistent ordering\n",
    "def flatten_params(params):\n",
    "    return torch.cat([params[name].view(-1) for name in sorted(params.keys())])\n",
    "\n",
    "# Unflatten parameters with consistent ordering\n",
    "def unflatten_params(flat_params, param_shapes, device):\n",
    "    params = {}\n",
    "    idx = 0\n",
    "    for name in sorted(param_shapes.keys()):\n",
    "        shape = param_shapes[name]\n",
    "        size = torch.prod(torch.tensor(shape)).item()\n",
    "        params[name] = flat_params[idx:idx+size].view(shape).to(device)\n",
    "        idx += size\n",
    "    return params\n",
    "\n",
    "# Model B\n",
    "class ModelB(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(ModelB, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data transformations\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Download and prepare the MNIST dataset\n",
    "full_train_dataset = datasets.MNIST(root='../data', train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='../data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Split the full training dataset into training and validation sets\n",
    "train_size = int(0.9 * len(full_train_dataset))  # 90% for training\n",
    "val_size = len(full_train_dataset) - train_size  # 10% for validation\n",
    "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training, validation, and testing\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize Model A's parameters\n",
    "params_A = initialize_model_A_params(device)\n",
    "\n",
    "# Get shapes of Model A's parameters\n",
    "param_shapes = {name: param.shape for name, param in params_A.items()}\n",
    "\n",
    "# Flatten Model A's parameters\n",
    "theta_A_flat = flatten_params(params_A)\n",
    "\n",
    "# Create Model B\n",
    "input_size = theta_A_flat.numel()\n",
    "output_size = input_size  # Output size is the same as input size\n",
    "model_B = ModelB(input_size, output_size).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer_B = optim.Adam(model_B.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/844], Loss: 0.4113, Training Accuracy: 84.38%\n",
      "Epoch [1/5], Step [200/844], Loss: 0.1275, Training Accuracy: 96.88%\n",
      "Epoch [1/5], Step [300/844], Loss: 0.1088, Training Accuracy: 96.88%\n",
      "Epoch [1/5], Step [400/844], Loss: 0.4814, Training Accuracy: 87.50%\n",
      "Epoch [1/5], Step [500/844], Loss: 0.1943, Training Accuracy: 96.88%\n",
      "Epoch [1/5], Step [600/844], Loss: 0.1136, Training Accuracy: 96.88%\n",
      "Epoch [1/5], Step [700/844], Loss: 0.1551, Training Accuracy: 96.88%\n",
      "Epoch [1/5], Step [800/844], Loss: 0.0600, Training Accuracy: 98.44%\n",
      "*** Epoch [1/5] Validation Accuracy: 97.02% ***\n",
      "\n",
      "Epoch [2/5], Step [100/844], Loss: 0.0937, Training Accuracy: 98.44%\n",
      "Epoch [2/5], Step [200/844], Loss: 0.1912, Training Accuracy: 95.31%\n",
      "Epoch [2/5], Step [300/844], Loss: 0.0376, Training Accuracy: 98.44%\n",
      "Epoch [2/5], Step [400/844], Loss: 0.1044, Training Accuracy: 98.44%\n",
      "Epoch [2/5], Step [500/844], Loss: 0.0508, Training Accuracy: 98.44%\n",
      "Epoch [2/5], Step [600/844], Loss: 0.0644, Training Accuracy: 96.88%\n",
      "Epoch [2/5], Step [700/844], Loss: 0.1058, Training Accuracy: 98.44%\n",
      "Epoch [2/5], Step [800/844], Loss: 0.1085, Training Accuracy: 95.31%\n",
      "*** Epoch [2/5] Validation Accuracy: 97.30% ***\n",
      "\n",
      "Epoch [3/5], Step [100/844], Loss: 0.3235, Training Accuracy: 89.06%\n",
      "Epoch [3/5], Step [200/844], Loss: 0.0955, Training Accuracy: 98.44%\n",
      "Epoch [3/5], Step [300/844], Loss: 0.0252, Training Accuracy: 100.00%\n",
      "Epoch [3/5], Step [400/844], Loss: 0.0849, Training Accuracy: 98.44%\n",
      "Epoch [3/5], Step [500/844], Loss: 0.0038, Training Accuracy: 100.00%\n",
      "Epoch [3/5], Step [600/844], Loss: 0.0988, Training Accuracy: 96.88%\n",
      "Epoch [3/5], Step [700/844], Loss: 0.0170, Training Accuracy: 100.00%\n",
      "Epoch [3/5], Step [800/844], Loss: 0.1193, Training Accuracy: 96.88%\n",
      "*** Epoch [3/5] Validation Accuracy: 97.70% ***\n",
      "\n",
      "Epoch [4/5], Step [100/844], Loss: 0.1385, Training Accuracy: 95.31%\n",
      "Epoch [4/5], Step [200/844], Loss: 0.0912, Training Accuracy: 95.31%\n",
      "Epoch [4/5], Step [300/844], Loss: 0.0348, Training Accuracy: 98.44%\n",
      "Epoch [4/5], Step [400/844], Loss: 0.1208, Training Accuracy: 95.31%\n",
      "Epoch [4/5], Step [500/844], Loss: 0.0799, Training Accuracy: 96.88%\n",
      "Epoch [4/5], Step [600/844], Loss: 0.0424, Training Accuracy: 98.44%\n",
      "Epoch [4/5], Step [700/844], Loss: 0.0238, Training Accuracy: 100.00%\n",
      "Epoch [4/5], Step [800/844], Loss: 0.1114, Training Accuracy: 93.75%\n",
      "*** Epoch [4/5] Validation Accuracy: 97.70% ***\n",
      "\n",
      "Epoch [5/5], Step [100/844], Loss: 0.0362, Training Accuracy: 98.44%\n",
      "Epoch [5/5], Step [200/844], Loss: 0.0525, Training Accuracy: 98.44%\n",
      "Epoch [5/5], Step [300/844], Loss: 0.0606, Training Accuracy: 96.88%\n",
      "Epoch [5/5], Step [400/844], Loss: 0.1324, Training Accuracy: 95.31%\n",
      "Epoch [5/5], Step [500/844], Loss: 0.0594, Training Accuracy: 96.88%\n",
      "Epoch [5/5], Step [600/844], Loss: 0.0265, Training Accuracy: 100.00%\n",
      "Epoch [5/5], Step [700/844], Loss: 0.0500, Training Accuracy: 96.88%\n",
      "Epoch [5/5], Step [800/844], Loss: 0.0270, Training Accuracy: 98.44%\n",
      "*** Epoch [5/5] Validation Accuracy: 97.17% ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer_B.zero_grad()\n",
    "        \n",
    "        # Pass params to B\n",
    "        theta_A_prime_flat = model_B(flatten_params(params_A))\n",
    "        \n",
    "        # Unflatten parameters\n",
    "        params_A_prime = unflatten_params(theta_A_prime_flat, param_shapes, device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_A_forward(inputs, params_A_prime)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        accuracy = correct / labels.size(0)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update Model B's parameters\n",
    "        optimizer_B.step()\n",
    "        \n",
    "        # Print loss and accuracy every 100 batches\n",
    "        if (batch_idx + 1) % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Step [{batch_idx+1}/{len(train_loader)}], '\n",
    "                  f'Loss: {loss.item():.4f}, '\n",
    "                  f'Training Accuracy: {accuracy * 100:.2f}%')\n",
    "    \n",
    "    \n",
    "    # validate\n",
    "    model_B.eval()\n",
    "    with torch.no_grad():\n",
    "        running_val_correct = 0\n",
    "        total_val = 0\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Pass flattened params to B\n",
    "            theta_A_prime_flat = model_B(flatten_params(params_A))\n",
    "            \n",
    "            # Unflatten parameters\n",
    "            params_A_prime = unflatten_params(theta_A_prime_flat, param_shapes, device)\n",
    "            \n",
    "            # Forward pass with updated parameters\n",
    "            outputs = model_A_forward(inputs, params_A_prime)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            running_val_correct += correct\n",
    "            total_val += labels.size(0)\n",
    "        \n",
    "        val_accuracy = (running_val_correct / total_val) * 100\n",
    "        print(f'*** Epoch [{epoch+1}/{num_epochs}] Validation Accuracy: {val_accuracy:.2f}% ***\\n')\n",
    "        \n",
    "    \n",
    "    # Update Model A's parameters for the next epoch\n",
    "    # Detach to prevent the graph from growing\n",
    "    params_A = {name: param.detach() for name, param in params_A_prime.items()}\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
